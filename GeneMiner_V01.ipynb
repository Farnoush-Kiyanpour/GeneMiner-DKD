{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c614ca-ed03-4ef3-99e0-847626e534ed",
   "metadata": {},
   "source": [
    "GeneMiner: Transformer-based Gene & Protein Mining from DKD Literature\n",
    "---------------------------------------------------------------------\n",
    "This Jupyter notebook implements the GeneMiner pipeline as described in the manuscript:\n",
    "- **Manuscript Title**: GeneMiner: A Transformer-Based Framework for Knowledge Extraction of Genes and Proteins from Diabetic Kidney Disease Literature\n",
    "- **Authors**: Farnoush Kiyanpour, Mahdi Kalani, AmirSepehr Saffari, Yousof Gheisari*\n",
    "- **Purpose**: Automated extraction, classification, and normalization of gene/protein mentions from DKD-related PubMed abstracts using transformer models (BioBERT, PubMedBERT) and hierarchical normalization (HGNC, MyGene, Wikipedia).\n",
    "\n",
    "The pipeline includes:\n",
    "1. PubMed abstract relevance classification (BioBERT)\n",
    "2. Gene/Protein NER (PubMedBERT / BENT)\n",
    "3. Entity normalization (HGNC + MyGene + Wikipedia fallback)\n",
    "4. Reproducible output generation for downstream analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5edfb9-8a64-47bd-b1db-93aa08f8da7d",
   "metadata": {},
   "source": [
    "0. Global Configuration:\n",
    "**Manuscript Reference**: Section \"Method\" – Implementation details, reproducibility, and environment setup.\n",
    "\n",
    "This cell sets up the Python environment, imports necessary libraries, and configures global parameters such as random seeds and device settings for reproducible execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d101b14-7a7a-4beb-b9c7-fb6677b22652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab021d5-089b-45b3-aed4-bb3ff23b7b53",
   "metadata": {},
   "source": [
    "1. Data Utilities:\n",
    "**Manuscript Reference**: Section \"Literature Retrieval and Annotation\" – Data preprocessing and dataset preparation.\n",
    "\n",
    "This section includes functions for text cleaning and dataset creation for transformer-based classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a351a2-f32e-4ff2-afd6-a4bb1546872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Minimal biomedical-safe text normalization.\"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "class AbstractDataset(Dataset):\n",
    "    \"\"\"Dataset for relevance classification.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_len\n",
    "        )\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f334ea6-e795-4048-bd58-413bc0a87d08",
   "metadata": {},
   "source": [
    "2. Relevance Classification (BioBERT):\n",
    "**Manuscript Reference**: Section \"Relevance Classification\" – Fine-tuning BioBERT for binary classification of relevant vs. irrelevant DKD abstracts.\n",
    "\n",
    "This module trains a BioBERT model to filter out non-relevant abstracts before entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707add0e-26b7-42bd-b9b7-2744239f45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_relevance_classifier(\n",
    "    train_texts, train_labels,\n",
    "    val_texts, val_labels,\n",
    "    output_dir=\"biobert_relevance\"\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"dmis-lab/biobert-v1.1\",\n",
    "        num_labels=2\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    train_ds = AbstractDataset(train_texts, train_labels, tokenizer)\n",
    "    val_ds = AbstractDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,\n",
    "        seed=SEED,\n",
    "        logging_steps=100\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69633dfb-0f5e-4dcb-b9b3-d7d0d864da80",
   "metadata": {},
   "source": [
    "3. Named-Entity Recognition (NER):\n",
    "**Manuscript Reference**: Section \"Named-Entity Recognition (NER)\" – Using BENT-PubMedBERT for gene/protein extraction.\n",
    "\n",
    "This module loads a pre-trained NER pipeline and extracts gene/protein mentions from relevant abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517cb4a-09d9-4398-a7c5-8e3edcd7aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_pipeline():\n",
    "    \"\"\"\n",
    "    BENT-PubMedBERT NER model for gene/protein extraction.\n",
    "    \"\"\"\n",
    "    ner = pipeline(\n",
    "        \"token-classification\",\n",
    "        model=\"pruas/BENT-PubMedBERT-NER-Gene\",\n",
    "        tokenizer=\"pruas/BENT-PubMedBERT-NER-Gene\",\n",
    "        aggregation_strategy=\"simple\",\n",
    "        device=0 if DEVICE == \"cuda\" else -1\n",
    "    )\n",
    "    return ner\n",
    "\n",
    "\n",
    "def extract_entities(texts: List[str], ner_pipeline) -> List[Dict]:\n",
    "    \"\"\"Extract gene/protein mentions from abstracts.\"\"\"\n",
    "    records = []\n",
    "    for pmid, text in texts:\n",
    "        entities = ner_pipeline(text)\n",
    "        for ent in entities:\n",
    "            records.append({\n",
    "                \"PMID\": pmid,\n",
    "                \"mention\": ent[\"word\"],\n",
    "                \"start\": ent[\"start\"],\n",
    "                \"end\": ent[\"end\"]\n",
    "            })\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376123b-2907-4796-b3c4-57c1b15223f0",
   "metadata": {},
   "source": [
    "4. Entity Normalization\n",
    "**Manuscript Reference**: Section \"Entity Normalization\" – Hierarchical mapping to HGNC-approved symbols using HGNC, MyGene, and Wikipedia.\n",
    "\n",
    "This module normalizes extracted entities to standard gene symbols using a tiered approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8fc56-719a-4aae-8ed5-806ab5123c3b",
   "metadata": {},
   "source": [
    "4a. Character-Level Normalization (Manuscript-Compliant)\n",
    "**Manuscript Reference**: Described in the normalization section – handling Greek letters, hyphens, and case standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ca3aa-20c4-4aee-9c03-6c2157345fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREEK_MAP = {\n",
    "    \"α\": \"alpha\", \"β\": \"beta\", \"γ\": \"gamma\",\n",
    "    \"δ\": \"delta\", \"κ\": \"kappa\", \"μ\": \"mu\",\n",
    "    \"τ\": \"tau\", \"ω\": \"omega\"\n",
    "}\n",
    "\n",
    "def normalize_characters(symbol: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply character-level normalization:\n",
    "    - Greek letter conversion\n",
    "    - Hyphen / underscore removal\n",
    "    - Uppercasing\n",
    "    \"\"\"\n",
    "    s = symbol\n",
    "    for g, latin in GREEK_MAP.items():\n",
    "        s = s.replace(g, latin)\n",
    "    s = re.sub(r\"[-_/]\", \"\", s)\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db9a0d-99e8-46db-88f6-0ad86c95da32",
   "metadata": {},
   "source": [
    "4b. Wikipedia-Assisted Disambiguation (Auxiliary, Flagged):\n",
    "**Manuscript Reference**: Auxiliary step used only when HGNC and MyGene fail; all matches flagged for manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a8cac-f65a-48e8-9f1c-882f4371abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_API = \"https://en.wikipedia.org/api/rest_v1/page/summary/{}\"\n",
    "\n",
    "def wikipedia_lookup(term: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Auxiliary Wikipedia title lookup for unresolved gene symbols.\n",
    "    Used ONLY when HGNC and MyGene fail.\n",
    "    All matches are flagged for manual curator verification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(WIKI_API.format(term), timeout=5)\n",
    "        if r.status_code != 200:\n",
    "            return {}\n",
    "        data = r.json()\n",
    "\n",
    "        if \"gene\" in data.get(\"description\", \"\").lower():\n",
    "            return {\n",
    "                \"wiki_title\": data.get(\"title\"),\n",
    "                \"wiki_description\": data.get(\"description\"),\n",
    "                \"source\": \"Wikipedia\"\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601e5e0-8adc-4b94-a221-680efff89205",
   "metadata": {},
   "source": [
    "4c. Entity Normalization (Hierarchical Mapping):\n",
    "**Manuscript Reference**: Primary normalization workflow – HGNC → MyGene → Wikipedia fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa35061-3cef-4655-9154-a9f3049ee057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_entities(entity_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Hierarchical entity normalization:\n",
    "    1) HGNC primary mapping\n",
    "    2) Character-level normalization\n",
    "    3) MyGene enrichment\n",
    "    4) Wikipedia-assisted disambiguation (flagged)\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "\n",
    "    for _, row in entity_df.iterrows():\n",
    "        raw_mention = row[\"mention\"]\n",
    "        mention = normalize_characters(raw_mention)\n",
    "\n",
    "        # --- Primary: HGNC\n",
    "        hgnc = normalize_hgnc(mention)\n",
    "        if hgnc:\n",
    "            normalized.append({\n",
    "                **row,\n",
    "                \"HGNC\": hgnc.get(\"symbol\"),\n",
    "                \"source\": \"HGNC\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # --- Secondary: MyGene\n",
    "        mg = normalize_mygene(mention)\n",
    "        if mg:\n",
    "            normalized.append({\n",
    "                **row,\n",
    "                \"HGNC\": mg.get(\"symbol\"),\n",
    "                \"source\": \"MyGene\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # --- Auxiliary: Wikipedia (flagged, non-quantitative)\n",
    "        wiki = wikipedia_lookup(mention)\n",
    "        if wiki:\n",
    "            normalized.append({\n",
    "                **row,\n",
    "                \"HGNC\": None,\n",
    "                \"source\": \"Wikipedia\",\n",
    "                \"wiki_title\": wiki.get(\"wiki_title\"),\n",
    "                \"flag_manual_review\": True\n",
    "            })\n",
    "\n",
    "        time.sleep(0.2)  # API rate limiting\n",
    "\n",
    "    return pd.DataFrame(normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da7a67-ab58-4815-bfb9-fb7b18234c24",
   "metadata": {},
   "source": [
    "5. End-to-End Execution:\n",
    "**Manuscript Reference**: Section \"Method\" – Integrated pipeline from classification to normalization.\n",
    "\n",
    "This function runs the complete GeneMiner pipeline: relevance filtering, NER, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2b94f-2801-4b9c-af63-4909c2a4c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_geneminer(\n",
    "    abstracts_df: pd.DataFrame,\n",
    "    relevance_model_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    abstracts_df columns:\n",
    "    - PMID\n",
    "    - text\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Load relevance classifier\n",
    "    tokenizer = AutoTokenizer.from_pretrained(relevance_model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        relevance_model_dir\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        abstracts_df[\"text\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    abstracts_df[\"relevant\"] = preds\n",
    "    relevant_df = abstracts_df[abstracts_df[\"relevant\"] == 1]\n",
    "\n",
    "    # ---- NER\n",
    "    ner = load_ner_pipeline()\n",
    "    texts = list(zip(relevant_df[\"PMID\"], relevant_df[\"text\"]))\n",
    "    entities = extract_entities(texts, ner)\n",
    "\n",
    "    entity_df = pd.DataFrame(entities)\n",
    "\n",
    "    # ---- Normalization\n",
    "    normalized_df = normalize_entities(entity_df)\n",
    "\n",
    "    return normalized_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c322d-3861-4f0b-ab8f-f9dc8f4756e8",
   "metadata": {},
   "source": [
    "6. Output Saving:\n",
    "7. **Manuscript Reference**: Section \"Reproducibility and FAIR Compliance\" – Saving results for downstream analysis.\n",
    "\n",
    "This module saves normalized gene lists and frequency tables in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5b724-4d12-4d29-835d-65569f1107ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(df: pd.DataFrame, outdir=\"results\"):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    df.to_csv(f\"{outdir}/GeneMiner_Normalized_Genes.csv\", index=False)\n",
    "    df.groupby(\"HGNC\").size().sort_values(ascending=False).to_csv(\n",
    "        f\"{outdir}/Gene_Frequency.csv\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
